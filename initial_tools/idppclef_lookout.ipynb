{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# [IDPP CLEF Challlenge](http://brainteaser.dei.unipd.it/challenges/idpp2023/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import lightgbm\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET = \"datasetA\"\n",
    "DATASET_DIR = f\"../data/{DATASET}_train\"\n",
    "\n",
    "ID_FEAT = \"patient_id\"\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load and Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filenames_in_folder(dir_path):\n",
    "    file_names = []\n",
    "    for _, __, f_names in os.walk(dir_path):\n",
    "        for file_name in f_names:\n",
    "            file_names.append(file_name)\n",
    "        break\n",
    "    return file_names\n",
    "\n",
    "def read_dfs(dir_path):\n",
    "    file_names = filenames_in_folder(dir_path)\n",
    "    dfs = {file_name.removesuffix(\".csv\"): pd.read_csv(os.path.join(dir_path, file_name)) for file_name in file_names if file_name.endswith(\"csv\")}\n",
    "    return dfs\n",
    "\n",
    "def merge_csv_in_dir(dfs, dataset):\n",
    "\n",
    "    def transpose_cols_to_rows_by_uniques(orig_df, id_feat, other_feature):\n",
    "        transposed_df = pd.DataFrame(orig_df.groupby(id_feat)[other_feature].apply(lambda x: x.values).values.tolist(), index=orig_df[id_feat].unique())\n",
    "        transposed_df.columns = [f'{other_feature}_{i:02d}' for i in range(1, len(transposed_df.columns) + 1)]\n",
    "        return transposed_df\n",
    "\n",
    "    def transpose_cols_to_rows_by_1st_unique(orig_df, id_feat, other_feature):\n",
    "        transposed_df = pd.DataFrame(orig_df.groupby(id_feat)[other_feature].apply(lambda x: x.values[0]),\n",
    "                         index=orig_df[id_feat].unique())\n",
    "        return transposed_df\n",
    "\n",
    "    def transpose_df_by_uniques(orig_df, id_feat, time_series_feats, one_occurrence_feats, sort_by_this_feat):\n",
    "        orig_df = orig_df.sort_values(by=[ID_FEAT, sort_by_this_feat])\n",
    "        ts_dfs = [transpose_cols_to_rows_by_uniques(orig_df, id_feat, ts_feat) for ts_feat in time_series_feats]\n",
    "        oo_dfs = [transpose_cols_to_rows_by_1st_unique(orig_df, id_feat, oo_feat) for oo_feat in one_occurrence_feats]\n",
    "\n",
    "        out_df = pd.concat([*oo_dfs, *ts_dfs], axis=1)\n",
    "        out_df.reset_index(names=id_feat, inplace=True)\n",
    "        return out_df\n",
    "\n",
    "    merged_df = pd.merge(dfs[f\"{dataset}_train-static-vars\"], dfs[f\"{dataset}_train-outcomes\"],\n",
    "                         on=\"patient_id\", how=\"outer\")\n",
    "\n",
    "    relapses_df = dfs[f\"{dataset}_train-relapses\"]\n",
    "    ts_feats = [\"delta_relapse_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_relapse_time0\"\n",
    "    relapses_df = transpose_df_by_uniques(relapses_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "    ms_type_df = dfs[f\"{dataset}_train-ms-type\"]\n",
    "    ts_feats = [\"multiple_sclerosis_type\", \"delta_observation_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_observation_time0\"\n",
    "    ms_type_df = transpose_df_by_uniques(ms_type_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "    mri_df = dfs[f\"{dataset}_train-mri\"]\n",
    "    ts_feats = [\"mri_area_label\", \"lesions_T1\", \"lesions_T1_gadolinium\", \"number_of_lesions_T1_gadolinium\",\n",
    "                \"new_or_enlarged_lesions_T2\", \"number_of_new_or_enlarged_lesions_T2\", \"lesions_T2\", \"number_of_total_lesions_T2\", \"delta_mri_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_mri_time0\"\n",
    "    mri_df = transpose_df_by_uniques(mri_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "    evoked_p_df = dfs[f\"{dataset}_train-evoked-potentials\"]\n",
    "    ts_feats = [\"altered_potential\", \"potential_value\", \"location\", \"delta_evoked_potential_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_evoked_potential_time0\"\n",
    "    evoked_p_df = transpose_df_by_uniques(evoked_p_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "\n",
    "    edss_df = dfs[f\"{dataset}_train-edss\"]\n",
    "    ts_feats = [\"edss_as_evaluated_by_clinician\", \"delta_edss_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_edss_time0\"\n",
    "    edss_df = transpose_df_by_uniques(edss_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "    grouped_dfs = [edss_df, relapses_df, ms_type_df, evoked_p_df, mri_df]\n",
    "    for df in grouped_dfs:\n",
    "        merged_df = pd.merge(merged_df, df, on=[ID_FEAT, \"centre\"], how=\"outer\")\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfs = read_dfs(DATASET_DIR)\n",
    "df = merge_csv_in_dir(dfs, DATASET)\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "print(df.columns.values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns.to_series().groupby(df.dtypes).groups)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "missing_values = ((df.isnull().sum() * 100 / len(df)).sort_values(ascending=False))\n",
    "print(\"Missing value rate:\\n\", missing_values.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.displot(df[\"outcome_time\"],kde=True, legend=True)\n",
    "sns.displot(df[\"outcome_occurred\"],kde=True, legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# corr_mat = df[continuous_values].corr()\n",
    "# f, ax = plt.subplots(figsize=(20, 20))\n",
    "# sns.heatmap(corr_mat,  vmax=1, cmap=\"viridis\", square=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Preprocessing\n"
    }
   },
   "outputs": [],
   "source": [
    "def collapse_ts_feature_cols(df, feature, start_idx, end_idx=None):\n",
    "    selected_cols = [col_name for col_name in df.columns.values.tolist() if col_name.startswith(feature)]\n",
    "    if end_idx:\n",
    "        cols_to_collapse = [col for col in selected_cols if (start_idx <= int(col[-2:] < end_idx))]\n",
    "        new_feat_name = f\"{feature}_{start_idx}-{end_idx}\"\n",
    "    else:\n",
    "        cols_to_collapse = [col for col in selected_cols if (start_idx <= int(col[-2:]))]\n",
    "        new_feat_name = f\"{feature}_{start_idx}+\"\n",
    "\n",
    "    df[new_feat_name] = df[cols_to_collapse].isna().all(axis=1)\n",
    "    df[new_feat_name] = df[new_feat_name].astype(int)\n",
    "    df = df.drop(cols_to_collapse, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feats_to_be_collapsed = [(\"new_or_enlarged_lesions_T2\", 5, None),\n",
    "                         (\"number_of_new_or_enlarged_lesions_T2\",   5, None),\n",
    "                         (\"altered_potential\", 9, None),\n",
    "                         (\"potential_value\", 9, None),\n",
    "                         (\"delta_relapse_time0\", 3, None),\n",
    "                         (\"mri_area_label\", 6, None),\n",
    "                         (\"delta_mri_time0\", 6, None),\n",
    "                         (\"lesions_T1\", 3, None),\n",
    "                         (\"lesions_T2\", 3, None),\n",
    "                         (\"delta_evoked_potential_time0\", 9, None),\n",
    "                         (\"lesions_T1_gadolinium\", 5, None),\n",
    "                         (\"number_of_lesions_T1_gadolinium\", 6, None),\n",
    "                         (\"edss_as_evaluated_by_clinician\", 11, None),\n",
    "                         (\"location\", 9, None),\n",
    "                         (\"delta_edss_time0\", 10, None),\n",
    "                         (\"number_of_total_lesions_T2\", 3, None)]\n",
    "\n",
    "def collapse_cols(df, feats_to_be_collapsed):\n",
    "    for feat in feats_to_be_collapsed:\n",
    "        df = collapse_ts_feature_cols(df, *feat)\n",
    "    return df\n",
    "\n",
    "df = collapse_cols(df, feats_to_be_collapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "missing_values = ((df.isnull().sum() * 100 / len(df)).sort_values(ascending=False))\n",
    "print(\"Missing value rate:\\n\", missing_values.to_string())\n",
    "print(df.columns.to_series().groupby(df.dtypes).groups)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "values = ['outcome_occurred', \"outcome_time\",'ms_in_pediatric_age', 'spinal_cord_symptom', 'brainstem_symptom', 'eye_symptom', 'supratentorial_symptom','new_or_enlarged_lesions_T2_5+', 'number_of_new_or_enlarged_lesions_T2_5+', 'altered_potential_9+', 'potential_value_9+', 'delta_relapse_time0_3+', 'mri_area_label_6+', 'delta_mri_time0_6+', 'lesions_T1_3+', 'lesions_T2_3+', 'delta_evoked_potential_time0_9+', 'lesions_T1_gadolinium_5+', 'number_of_lesions_T1_gadolinium_6+', 'edss_as_evaluated_by_clinician_11+', 'location_9+', 'delta_edss_time0_10+', 'number_of_total_lesions_T2_3+','age_at_onset', 'time_since_onset', 'diagnostic_delay', 'delta_relapse_time0_01', 'delta_relapse_time0_02', 'delta_observation_time0_01', 'delta_observation_time0_02', 'number_of_lesions_T1_gadolinium_01', 'number_of_lesions_T1_gadolinium_02', 'number_of_lesions_T1_gadolinium_03', 'number_of_lesions_T1_gadolinium_04', 'number_of_lesions_T1_gadolinium_05', 'number_of_new_or_enlarged_lesions_T2_01', 'number_of_new_or_enlarged_lesions_T2_02', 'number_of_new_or_enlarged_lesions_T2_03', 'number_of_new_or_enlarged_lesions_T2_04', 'delta_mri_time0_01', 'delta_mri_time0_02', 'delta_mri_time0_03', 'delta_mri_time0_04', 'delta_mri_time0_05', 'delta_evoked_potential_time0_01', 'delta_evoked_potential_time0_02', 'delta_evoked_potential_time0_03', 'delta_evoked_potential_time0_04', 'delta_evoked_potential_time0_05', 'delta_evoked_potential_time0_06', 'delta_evoked_potential_time0_07', 'delta_evoked_potential_time0_08', 'edss_as_evaluated_by_clinician_01', 'edss_as_evaluated_by_clinician_02', 'edss_as_evaluated_by_clinician_03', 'edss_as_evaluated_by_clinician_04', 'edss_as_evaluated_by_clinician_05', 'edss_as_evaluated_by_clinician_06', 'edss_as_evaluated_by_clinician_07', 'edss_as_evaluated_by_clinician_08', 'edss_as_evaluated_by_clinician_09', 'edss_as_evaluated_by_clinician_10', 'delta_edss_time0_01', 'delta_edss_time0_02', 'delta_edss_time0_03', 'delta_edss_time0_04', 'delta_edss_time0_05', 'delta_edss_time0_06', 'delta_edss_time0_07', 'delta_edss_time0_08', 'delta_edss_time0_09']\n",
    "corr_mat = df[values].corr()\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "sns.heatmap(corr_mat,  vmax=1, cmap=\"viridis\", square=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import fastcore\n",
    "from fastai.tabular.all import RandomSplitter, range_of, TabularPandas, Categorify, FillMissing, Normalize\n",
    "\n",
    "cols_typed = {\"bool\": ['ms_in_pediatric_age', 'spinal_cord_symptom', 'brainstem_symptom', 'eye_symptom', 'supratentorial_symptom'],\n",
    "     \"int32\": ['new_or_enlarged_lesions_T2_5+', 'number_of_new_or_enlarged_lesions_T2_5+', 'altered_potential_9+', 'potential_value_9+', 'delta_relapse_time0_3+', 'mri_area_label_6+', 'delta_mri_time0_6+', 'lesions_T1_3+', 'lesions_T2_3+', 'delta_evoked_potential_time0_9+', 'lesions_T1_gadolinium_5+', 'number_of_lesions_T1_gadolinium_6+', 'edss_as_evaluated_by_clinician_11+', 'location_9+', 'delta_edss_time0_10+', 'number_of_total_lesions_T2_3+'],\n",
    "     \"int64\": ['age_at_onset', 'time_since_onset'],\n",
    "     \"float64\": ['diagnostic_delay', 'delta_relapse_time0_01', 'delta_relapse_time0_02', 'delta_observation_time0_01', 'delta_observation_time0_02', 'number_of_lesions_T1_gadolinium_01', 'number_of_lesions_T1_gadolinium_02', 'number_of_lesions_T1_gadolinium_03', 'number_of_lesions_T1_gadolinium_04', 'number_of_lesions_T1_gadolinium_05', 'number_of_new_or_enlarged_lesions_T2_01', 'number_of_new_or_enlarged_lesions_T2_02', 'number_of_new_or_enlarged_lesions_T2_03', 'number_of_new_or_enlarged_lesions_T2_04', 'delta_mri_time0_01', 'delta_mri_time0_02', 'delta_mri_time0_03', 'delta_mri_time0_04', 'delta_mri_time0_05', 'delta_evoked_potential_time0_01', 'delta_evoked_potential_time0_02', 'delta_evoked_potential_time0_03', 'delta_evoked_potential_time0_04', 'delta_evoked_potential_time0_05', 'delta_evoked_potential_time0_06', 'delta_evoked_potential_time0_07', 'delta_evoked_potential_time0_08', 'edss_as_evaluated_by_clinician_01', 'edss_as_evaluated_by_clinician_02', 'edss_as_evaluated_by_clinician_03', 'edss_as_evaluated_by_clinician_04', 'edss_as_evaluated_by_clinician_05', 'edss_as_evaluated_by_clinician_06', 'edss_as_evaluated_by_clinician_07', 'edss_as_evaluated_by_clinician_08', 'edss_as_evaluated_by_clinician_09', 'edss_as_evaluated_by_clinician_10', 'delta_edss_time0_01', 'delta_edss_time0_02', 'delta_edss_time0_03', 'delta_edss_time0_04', 'delta_edss_time0_05', 'delta_edss_time0_06', 'delta_edss_time0_07', 'delta_edss_time0_08', 'delta_edss_time0_09'],\n",
    "     \"object\": [ 'sex', 'residence_classification', 'ethnicity', 'other_symptoms', 'centre', 'multiple_sclerosis_type_01', 'multiple_sclerosis_type_02', 'mri_area_label_01', 'mri_area_label_02', 'mri_area_label_03', 'mri_area_label_04', 'mri_area_label_05', 'lesions_T1_01', 'lesions_T1_02', 'lesions_T1_gadolinium_01', 'lesions_T1_gadolinium_02', 'new_or_enlarged_lesions_T2_01', 'new_or_enlarged_lesions_T2_02', 'new_or_enlarged_lesions_T2_03', 'new_or_enlarged_lesions_T2_04', 'lesions_T2_01', 'lesions_T2_02', 'number_of_total_lesions_T2_01', 'number_of_total_lesions_T2_02', 'altered_potential_01', 'altered_potential_02', 'altered_potential_03', 'altered_potential_04', 'altered_potential_05', 'altered_potential_06', 'altered_potential_07', 'altered_potential_08', 'potential_value_01', 'potential_value_02', 'potential_value_03', 'potential_value_04', 'potential_value_05', 'potential_value_06', 'potential_value_07', 'potential_value_08', 'location_01', 'location_02', 'location_03', 'location_04', 'location_05', 'location_06', 'location_07', 'location_08']}\n",
    "\n",
    "cat_names = [*cols_typed[\"bool\"], *cols_typed[\"object\"]]\n",
    "cont_names = [*cols_typed[\"int32\"], *cols_typed[\"int64\"], *cols_typed[\"float64\"]]\n",
    "\n",
    "splits = RandomSplitter(valid_pct=0.2)(range_of(df))\n",
    "\n",
    "to = TabularPandas(df, procs=[Categorify, FillMissing],\n",
    "                   cat_names = cat_names,\n",
    "                   cont_names = cont_names,\n",
    "                   y_names='outcome_occurred',\n",
    "                   splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = to.train.xs, to.train.ys.values.ravel()\n",
    "X_valid, y_valid = to.valid.xs, to.valid.ys.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "svc = SVC()\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "gauss = GaussianNB()\n",
    "perceptron = Perceptron()\n",
    "linear_svc = LinearSVC()\n",
    "sgd = SGDClassifier()\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier(n_estimators=200, n_jobs=-1)\n",
    "\n",
    "xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                  colsample_bynode=1, colsample_bytree=1, eval_metric='mlogloss',\n",
    "                  gamma=0, gpu_id=-1, importance_type='gain',\n",
    "                  interaction_constraints='', learning_rate=0.300000012,\n",
    "                  max_delta_step=0, max_depth=10, min_child_weight=1,\n",
    "                  monotone_constraints='()', n_estimators=1000, n_jobs=20,\n",
    "                  num_parallel_tree=1, objective='binary:logistic', random_state=0,\n",
    "                  reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
    "                  tree_method='exact',\n",
    "                  validate_parameters=1, verbosity=None)\n",
    "\n",
    "lgm = lightgbm.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "                importance_type='split', learning_rate=0.1, max_depth=-1,\n",
    "                min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "                n_estimators=1000, n_jobs=-1, num_leaves=31, objective=None,\n",
    "                random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
    "                subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "\n",
    "models = [log_reg, svc, knn, gauss, perceptron, linear_svc, sgd, decision_tree, random_forest, xgb, lgm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc = round(model.score(X_train, y_train) * 100, 2)\n",
    "\n",
    "    val_acc = round(model.score(X_valid, y_valid) * 100, 2)\n",
    "    accuracies[model.__class__.__name__] = {\"train_acc\": train_acc, \"val_acc\": val_acc}\n",
    "\n",
    "classification_preds = lgm.predict(X_valid)\n",
    "acc_df = pd.DataFrame(accuracies).transpose().sort_values(\"val_acc\", ascending=False)\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(X_train.columns.values)\n",
    "coeff_df.columns = [\"Feature\"]\n",
    "coeff_df[\"Correlation\"] = pd.Series(models[0].coef_[0])\n",
    "\n",
    "coeff_df.sort_values(\"Correlation\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols_typed = {\"bool\": ['ms_in_pediatric_age', 'spinal_cord_symptom', 'brainstem_symptom', 'eye_symptom', 'supratentorial_symptom'],\n",
    "     \"int32\": ['new_or_enlarged_lesions_T2_5+', 'number_of_new_or_enlarged_lesions_T2_5+', 'altered_potential_9+', 'potential_value_9+', 'delta_relapse_time0_3+', 'mri_area_label_6+', 'delta_mri_time0_6+', 'lesions_T1_3+', 'lesions_T2_3+', 'delta_evoked_potential_time0_9+', 'lesions_T1_gadolinium_5+', 'number_of_lesions_T1_gadolinium_6+', 'edss_as_evaluated_by_clinician_11+', 'location_9+', 'delta_edss_time0_10+', 'number_of_total_lesions_T2_3+'],\n",
    "     \"int64\": ['age_at_onset', 'time_since_onset', \"outcome_occurred\"],\n",
    "     \"float64\": ['diagnostic_delay', 'outcome_time', 'delta_relapse_time0_01', 'delta_relapse_time0_02', 'delta_observation_time0_01', 'delta_observation_time0_02', 'number_of_lesions_T1_gadolinium_01', 'number_of_lesions_T1_gadolinium_02', 'number_of_lesions_T1_gadolinium_03', 'number_of_lesions_T1_gadolinium_04', 'number_of_lesions_T1_gadolinium_05', 'number_of_new_or_enlarged_lesions_T2_01', 'number_of_new_or_enlarged_lesions_T2_02', 'number_of_new_or_enlarged_lesions_T2_03', 'number_of_new_or_enlarged_lesions_T2_04', 'delta_mri_time0_01', 'delta_mri_time0_02', 'delta_mri_time0_03', 'delta_mri_time0_04', 'delta_mri_time0_05', 'delta_evoked_potential_time0_01', 'delta_evoked_potential_time0_02', 'delta_evoked_potential_time0_03', 'delta_evoked_potential_time0_04', 'delta_evoked_potential_time0_05', 'delta_evoked_potential_time0_06', 'delta_evoked_potential_time0_07', 'delta_evoked_potential_time0_08', 'edss_as_evaluated_by_clinician_01', 'edss_as_evaluated_by_clinician_02', 'edss_as_evaluated_by_clinician_03', 'edss_as_evaluated_by_clinician_04', 'edss_as_evaluated_by_clinician_05', 'edss_as_evaluated_by_clinician_06', 'edss_as_evaluated_by_clinician_07', 'edss_as_evaluated_by_clinician_08', 'edss_as_evaluated_by_clinician_09', 'edss_as_evaluated_by_clinician_10', 'delta_edss_time0_01', 'delta_edss_time0_02', 'delta_edss_time0_03', 'delta_edss_time0_04', 'delta_edss_time0_05', 'delta_edss_time0_06', 'delta_edss_time0_07', 'delta_edss_time0_08', 'delta_edss_time0_09'],\n",
    "     \"object\": [ 'sex', 'residence_classification', 'ethnicity', 'other_symptoms', 'centre', 'multiple_sclerosis_type_01', 'multiple_sclerosis_type_02', 'mri_area_label_01', 'mri_area_label_02', 'mri_area_label_03', 'mri_area_label_04', 'mri_area_label_05', 'lesions_T1_01', 'lesions_T1_02', 'lesions_T1_gadolinium_01', 'lesions_T1_gadolinium_02', 'new_or_enlarged_lesions_T2_01', 'new_or_enlarged_lesions_T2_02', 'new_or_enlarged_lesions_T2_03', 'new_or_enlarged_lesions_T2_04', 'lesions_T2_01', 'lesions_T2_02', 'number_of_total_lesions_T2_01', 'number_of_total_lesions_T2_02', 'altered_potential_01', 'altered_potential_02', 'altered_potential_03', 'altered_potential_04', 'altered_potential_05', 'altered_potential_06', 'altered_potential_07', 'altered_potential_08', 'potential_value_01', 'potential_value_02', 'potential_value_03', 'potential_value_04', 'potential_value_05', 'potential_value_06', 'potential_value_07', 'potential_value_08', 'location_01', 'location_02', 'location_03', 'location_04', 'location_05', 'location_06', 'location_07', 'location_08']}\n",
    "\n",
    "cat_names = [*cols_typed[\"bool\"], *cols_typed[\"object\"]]\n",
    "cont_names = [*cols_typed[\"int32\"], *cols_typed[\"int64\"], *cols_typed[\"float64\"]]\n",
    "\n",
    "splits = RandomSplitter(valid_pct=0.2)(range_of(df))\n",
    "\n",
    "to = TabularPandas(df, procs=[Categorify, FillMissing, Normalize],\n",
    "                   cat_names = cat_names,\n",
    "                   cont_names = cont_names,\n",
    "                   y_names='outcome_time',\n",
    "                   splits=splits)\n",
    "\n",
    "X_train, y_train = to.train.xs, to.train.ys.values.ravel()\n",
    "X_valid, y_valid = to.valid.xs, to.valid.ys.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# y_train = df[\"outcome_time\"]\n",
    "# train_df = df.drop([\"outcome_time\", \"patient_id\"], axis=1)\n",
    "\n",
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=43).get_n_splits(X_train.values)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10,\n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,\n",
    "                             learning_rate=0.05, max_depth=3,\n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_lgb = lightgbm.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "regressors = {\"Lasso\": lasso, \"ElasticNet\": ENet, \"Kernel Ridge\": KRR,\n",
    "              \"Gradient Boosting\": GBoost, \"XGBoost\": model_xgb,\n",
    "              \"LGBM\": model_lgb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for model_name, model in regressors.items():\n",
    "    score = rmsle_cv(model)\n",
    "    print(f\"{model_name} score: {score.mean():.4f} ({score.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sns.kdeplot(y_valid)\n",
    "# print(min(y_valid), max(y_valid), 15/(max(y_valid)-min(y_valid)))\n",
    "# tmp = 15/(max(y_valid)-min(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean, std = to.means[\"outcome_time\"], to.stds[\"outcome_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = regressors[\"Lasso\"]\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_valid)\n",
    "\n",
    "pred_df = pd.DataFrame({\"ground truth\":y_valid*std + mean, \"preds\": preds*std + mean,\n",
    "                        \"MSE\": ((y_valid*std + mean - (preds*std + mean))**2)/len(y_valid),\n",
    "                        \"MAE\": (abs(y_valid*std + mean - (preds*std + mean)))/len(y_valid)})\n",
    "print(f\"Total error:\\nMSE: {sum(pred_df['MSE'])}\\tMAE: {sum(pred_df['MAE'])}\")\n",
    "pred_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scores and output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TEAM_IDENTIFIER = uwb_T1a_metric_method\n",
    "# Harell's C-index: https://lifelines.readthedocs.io/en/latest/lifelines.utils.html\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "regression_preds = lasso.predict(X_valid)\n",
    "from lifelines.utils.concordance import concordance_index\n",
    "c_index = concordance_index(regression_preds, classification_preds)\n",
    "\n",
    "c_index\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET = \"datasetB\"\n",
    "DATASET_DIR = f\"../data/{DATASET}_train\"\n",
    "\n",
    "dfs = read_dfs(DATASET_DIR)\n",
    "df = merge_csv_in_dir(dfs, DATASET)\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "df = collapse_cols(df, feats_to_be_collapsed)\n",
    "\n",
    "cols_typed = {\"bool\": ['ms_in_pediatric_age', 'spinal_cord_symptom', 'brainstem_symptom', 'eye_symptom', 'supratentorial_symptom'],\n",
    "     \"int32\": ['new_or_enlarged_lesions_T2_5+', 'number_of_new_or_enlarged_lesions_T2_5+', 'altered_potential_9+', 'potential_value_9+', 'delta_relapse_time0_3+', 'mri_area_label_6+', 'delta_mri_time0_6+', 'lesions_T1_3+', 'lesions_T2_3+', 'delta_evoked_potential_time0_9+', 'lesions_T1_gadolinium_5+', 'number_of_lesions_T1_gadolinium_6+', 'edss_as_evaluated_by_clinician_11+', 'location_9+', 'delta_edss_time0_10+', 'number_of_total_lesions_T2_3+'],\n",
    "     \"int64\": ['age_at_onset', 'time_since_onset'],\n",
    "     \"float64\": ['diagnostic_delay', 'delta_relapse_time0_01', 'delta_relapse_time0_02', 'delta_observation_time0_01', 'delta_observation_time0_02', 'number_of_lesions_T1_gadolinium_01', 'number_of_lesions_T1_gadolinium_02', 'number_of_lesions_T1_gadolinium_03', 'number_of_lesions_T1_gadolinium_04', 'number_of_lesions_T1_gadolinium_05', 'number_of_new_or_enlarged_lesions_T2_01', 'number_of_new_or_enlarged_lesions_T2_02', 'number_of_new_or_enlarged_lesions_T2_03', 'number_of_new_or_enlarged_lesions_T2_04', 'delta_mri_time0_01', 'delta_mri_time0_02', 'delta_mri_time0_03', 'delta_mri_time0_04', 'delta_mri_time0_05', 'delta_evoked_potential_time0_01', 'delta_evoked_potential_time0_02', 'delta_evoked_potential_time0_03', 'delta_evoked_potential_time0_04', 'delta_evoked_potential_time0_05', 'delta_evoked_potential_time0_06', 'delta_evoked_potential_time0_07', 'delta_evoked_potential_time0_08', 'edss_as_evaluated_by_clinician_01', 'edss_as_evaluated_by_clinician_02', 'edss_as_evaluated_by_clinician_03', 'edss_as_evaluated_by_clinician_04', 'edss_as_evaluated_by_clinician_05', 'edss_as_evaluated_by_clinician_06', 'edss_as_evaluated_by_clinician_07', 'edss_as_evaluated_by_clinician_08', 'edss_as_evaluated_by_clinician_09', 'edss_as_evaluated_by_clinician_10', 'delta_edss_time0_01', 'delta_edss_time0_02', 'delta_edss_time0_03', 'delta_edss_time0_04', 'delta_edss_time0_05', 'delta_edss_time0_06', 'delta_edss_time0_07', 'delta_edss_time0_08', 'delta_edss_time0_09'],\n",
    "     \"object\": [ 'sex', 'residence_classification', 'ethnicity', 'other_symptoms', 'centre', 'multiple_sclerosis_type_01', 'multiple_sclerosis_type_02', 'mri_area_label_01', 'mri_area_label_02', 'mri_area_label_03', 'mri_area_label_04', 'mri_area_label_05', 'lesions_T1_01', 'lesions_T1_02', 'lesions_T1_gadolinium_01', 'lesions_T1_gadolinium_02', 'new_or_enlarged_lesions_T2_01', 'new_or_enlarged_lesions_T2_02', 'new_or_enlarged_lesions_T2_03', 'new_or_enlarged_lesions_T2_04', 'lesions_T2_01', 'lesions_T2_02', 'number_of_total_lesions_T2_01', 'number_of_total_lesions_T2_02', 'altered_potential_01', 'altered_potential_02', 'altered_potential_03', 'altered_potential_04', 'altered_potential_05', 'altered_potential_06', 'altered_potential_07', 'altered_potential_08', 'potential_value_01', 'potential_value_02', 'potential_value_03', 'potential_value_04', 'potential_value_05', 'potential_value_06', 'potential_value_07', 'potential_value_08', 'location_01', 'location_02', 'location_03', 'location_04', 'location_05', 'location_06', 'location_07', 'location_08']}\n",
    "\n",
    "cat_names = [*cols_typed[\"bool\"], *cols_typed[\"object\"]]\n",
    "cont_names = [*cols_typed[\"int32\"], *cols_typed[\"int64\"], *cols_typed[\"float64\"]]\n",
    "\n",
    "splits = RandomSplitter(valid_pct=0.2)(range_of(df))\n",
    "\n",
    "to = TabularPandas(df, procs=[Categorify, FillMissing],\n",
    "                   cat_names = cat_names,\n",
    "                   cont_names = cont_names,\n",
    "                   y_names='outcome_occurred',\n",
    "                   splits=splits)\n",
    "\n",
    "X_train, y_train = to.train.xs, to.train.ys.values.ravel()\n",
    "X_valid, y_valid = to.valid.xs, to.valid.ys.values.ravel()\n",
    "\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc = round(model.score(X_train, y_train) * 100, 2)\n",
    "\n",
    "    val_acc = round(model.score(X_valid, y_valid) * 100, 2)\n",
    "    accuracies[model.__class__.__name__] = {\"train_acc\": train_acc, \"val_acc\": val_acc}\n",
    "\n",
    "\n",
    "acc_df = pd.DataFrame(accuracies).transpose().sort_values(\"val_acc\", ascending=False)\n",
    "acc_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Test it model on datasetB\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
