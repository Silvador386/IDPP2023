{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# [IDPP CLEF Challlenge](http://brainteaser.dei.unipd.it/challenges/idpp2023/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import lightgbm\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = \"datasetB\"\n",
    "DATASET_DIR = f\"../data/{DATASET}_train\"\n",
    "ID_FEAT = \"patient_id\"\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filenames_in_folder(dir_path):\n",
    "    file_names = []\n",
    "    for _, __, f_names in os.walk(dir_path):\n",
    "        for file_name in f_names:\n",
    "            file_names.append(file_name)\n",
    "        break\n",
    "    return file_names\n",
    "\n",
    "\n",
    "def read_dfs(dir_path):\n",
    "    file_names = filenames_in_folder(dir_path)\n",
    "    dfs = {file_name.removesuffix(\".csv\"): pd.read_csv(os.path.join(dir_path, file_name)) for file_name in file_names if file_name.endswith(\"csv\")}\n",
    "    return dfs\n",
    "\n",
    "dfs = read_dfs(DATASET_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dfs_unique_patients(dfs):\n",
    "    out = {}\n",
    "    for name, df in dfs.items():\n",
    "        out[name] = len(df[ID_FEAT].unique())\n",
    "    return pd.DataFrame.from_dict(out, orient=\"index\", columns=[\"Unique patients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_dfs_complete(dfs, dataset):\n",
    "\n",
    "    def group_ts_column_to_list(orig_df, id_feat, other_feature):\n",
    "        unique_ids = orig_df[id_feat].unique()\n",
    "        grouped_ts_column_as_list = orig_df.groupby(id_feat)[other_feature].apply(lambda x: x.values).values.tolist()\n",
    "        new_df = pd.DataFrame.from_dict({id_feat: unique_ids,\n",
    "                                         other_feature: grouped_ts_column_as_list})\n",
    "        new_df = new_df.set_index(id_feat)\n",
    "        return new_df\n",
    "\n",
    "    def group_ts_column_select_first(orig_df, id_feat, other_feature):\n",
    "        unique_ids = orig_df[id_feat].unique()\n",
    "        first_values = orig_df.groupby(id_feat)[other_feature].apply(lambda x: x.values[0]).values\n",
    "\n",
    "        new_df = pd.DataFrame.from_dict({id_feat: unique_ids,\n",
    "                                         other_feature: first_values})\n",
    "        new_df = new_df.set_index(id_feat)\n",
    "        return new_df\n",
    "\n",
    "    def group_ts_df_by_id(orig_df, id_feat, time_series_feats, one_occurrence_feats, sort_by_this_feat):\n",
    "        orig_df = orig_df.sort_values(by=[ID_FEAT, sort_by_this_feat])\n",
    "        ts_dfs = [group_ts_column_to_list(orig_df, id_feat, ts_feat) for ts_feat in time_series_feats]\n",
    "        oo_dfs = [group_ts_column_select_first(orig_df, id_feat, oo_feat) for oo_feat in one_occurrence_feats]\n",
    "\n",
    "        out_df = pd.concat([*oo_dfs, *ts_dfs], axis=1)\n",
    "        out_df.reset_index(names=id_feat, inplace=True)\n",
    "        return out_df\n",
    "\n",
    "    merged_df = pd.merge(dfs[f\"{dataset}_train-static-vars\"], dfs[f\"{dataset}_train-outcomes\"],\n",
    "                         on=\"patient_id\", how=\"outer\")\n",
    "\n",
    "    relapses_df = dfs[f\"{dataset}_train-relapses\"]\n",
    "    ts_feats = [\"delta_relapse_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_relapse_time0\"\n",
    "    relapses_df = group_ts_df_by_id(relapses_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "    ms_type_df = dfs[f\"{dataset}_train-ms-type\"]\n",
    "    ts_feats = [\"multiple_sclerosis_type\", \"delta_observation_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_observation_time0\"\n",
    "    ms_type_df = group_ts_df_by_id(ms_type_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "    mri_df = dfs[f\"{dataset}_train-mri\"]\n",
    "    ts_feats = [\"mri_area_label\", \"lesions_T1\", \"lesions_T1_gadolinium\", \"number_of_lesions_T1_gadolinium\",\n",
    "                \"new_or_enlarged_lesions_T2\", \"number_of_new_or_enlarged_lesions_T2\", \"lesions_T2\", \"number_of_total_lesions_T2\", \"delta_mri_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_mri_time0\"\n",
    "    mri_df = group_ts_df_by_id(mri_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "    evoked_p_df = dfs[f\"{dataset}_train-evoked-potentials\"]\n",
    "    ts_feats = [\"altered_potential\", \"potential_value\", \"location\", \"delta_evoked_potential_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_evoked_potential_time0\"\n",
    "    evoked_p_df = group_ts_df_by_id(evoked_p_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "\n",
    "    edss_df = dfs[f\"{dataset}_train-edss\"]\n",
    "    ts_feats = [\"edss_as_evaluated_by_clinician\", \"delta_edss_time0\"]\n",
    "    oo_feats = [\"centre\"]\n",
    "    sort_by_this_feat = \"delta_edss_time0\"\n",
    "    edss_df = group_ts_df_by_id(edss_df, ID_FEAT, ts_feats, oo_feats, sort_by_this_feat)\n",
    "\n",
    "    grouped_dfs = [edss_df, relapses_df, ms_type_df, evoked_p_df, mri_df]\n",
    "    for df in grouped_dfs:\n",
    "        merged_df = pd.merge(merged_df, df, on=[ID_FEAT, \"centre\"], how=\"outer\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "dfs = read_dfs(DATASET_DIR)\n",
    "merged_df = merge_dfs_complete(dfs, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                  Unique patients\ndatasetB_train-edss                           510\ndatasetB_train-evoked-potentials              183\ndatasetB_train-mri                            303\ndatasetB_train-ms-type                        218\ndatasetB_train-outcomes                       510\ndatasetB_train-relapses                       284\ndatasetB_train-static-vars                    510\nmerged_df                                     510",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unique patients</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>datasetB_train-edss</th>\n      <td>510</td>\n    </tr>\n    <tr>\n      <th>datasetB_train-evoked-potentials</th>\n      <td>183</td>\n    </tr>\n    <tr>\n      <th>datasetB_train-mri</th>\n      <td>303</td>\n    </tr>\n    <tr>\n      <th>datasetB_train-ms-type</th>\n      <td>218</td>\n    </tr>\n    <tr>\n      <th>datasetB_train-outcomes</th>\n      <td>510</td>\n    </tr>\n    <tr>\n      <th>datasetB_train-relapses</th>\n      <td>284</td>\n    </tr>\n    <tr>\n      <th>datasetB_train-static-vars</th>\n      <td>510</td>\n    </tr>\n    <tr>\n      <th>merged_df</th>\n      <td>510</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"merged_df\"] = merged_df\n",
    "unique_patients = dfs_unique_patients(dfs)\n",
    "unique_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "{bool: ['ms_in_pediatric_age', 'spinal_cord_symptom', 'brainstem_symptom', 'eye_symptom', 'supratentorial_symptom'], int64: ['age_at_onset', 'time_since_onset', 'outcome_occurred'], float64: ['diagnostic_delay', 'outcome_time'], object: ['patient_id', 'sex', 'residence_classification', 'ethnicity', 'other_symptoms', 'centre', 'edss_as_evaluated_by_clinician', 'delta_edss_time0', 'delta_relapse_time0', 'multiple_sclerosis_type', 'delta_observation_time0', 'altered_potential', 'potential_value', 'location', 'delta_evoked_potential_time0', 'mri_area_label', 'lesions_T1', 'lesions_T1_gadolinium', 'number_of_lesions_T1_gadolinium', 'new_or_enlarged_lesions_T2', 'number_of_new_or_enlarged_lesions_T2', 'lesions_T2', 'number_of_total_lesions_T2', 'delta_mri_time0']}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns.to_series().groupby(merged_df.dtypes).groups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['female' 'female' 'female' 'male' 'female' 'female' 'female' 'male'\n 'male' 'female' 'female' 'female' 'male' 'male' 'female' 'female' 'male'\n 'female' 'female' 'male' 'female' 'female' 'female' 'female' 'male'\n 'female' 'male' 'female' 'male' 'male' 'male' 'male' 'male' 'female'\n 'female' 'male' 'male' 'male' 'female' 'female' 'female' 'female'\n 'female' 'male' 'male' 'female' 'female' 'male' 'male' 'female' 'female'\n 'female' 'male' 'female' 'male' 'male' 'female' 'male' 'male' 'female'\n 'female' 'male' 'male' 'male' 'female' 'female' 'female' 'male' 'female'\n 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'male' 'female'\n 'male' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'male'\n 'female' 'female' 'female' 'female' 'female' 'male' 'female' 'female'\n 'male' 'female' 'female' 'female' 'female' 'female' 'female' 'male'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'male'\n 'male' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'male' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'female' 'female' 'female' 'female' 'male' 'female'\n 'male' 'female' 'male' 'male' 'female' 'female' 'male' 'female' 'female'\n 'female' 'male' 'female' 'female' 'male' 'female' 'male' 'female' 'male'\n 'male' 'female' 'female' 'female' 'female' 'female' 'female' 'male'\n 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'female' 'male'\n 'female' 'male' 'female' 'female' 'male' 'female' 'female' 'male'\n 'female' 'male' 'female' 'male' 'male' 'male' 'female' 'female' 'female'\n 'female' 'female' 'female' 'male' 'female' 'male' 'female' 'female'\n 'female' 'male' 'female' 'female' 'male' 'male' 'female' 'female'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'female' 'female' 'female' 'female' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'male' 'female'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'male' 'female' 'male' 'female' 'female' 'male'\n 'female' 'female' 'female' 'male' 'male' 'female' 'male' 'male' 'female'\n 'female' 'male' 'female' 'female' 'female' 'female' 'male' 'female'\n 'female' 'male' 'female' 'female' 'female' 'male' 'female' 'male'\n 'female' 'male' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'female' 'male' 'female' 'female' 'female' 'male'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'male' 'female' 'female' 'female' 'female' 'male'\n 'female' 'male' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'male'\n 'female' 'male' 'male' 'female' 'female' 'female' 'female' 'male'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'female' 'female' 'female' 'male' 'male' 'female' 'female' 'male'\n 'female' 'male' 'male' 'male' 'male' 'female' 'female' 'female' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'female' 'male' 'female' 'female' 'male' 'female' 'male' 'female' 'male'\n 'female' 'female' 'female' 'male' 'female' 'male' 'female' 'male'\n 'female' 'female' 'female' 'male' 'female' 'female' 'female' 'male'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'male' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'male' 'female' 'female' 'male' 'female' 'female'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'male' 'female' 'male' 'female' 'male' 'female' 'male' 'male' 'female'\n 'female' 'male' 'female' 'female' 'female' 'female' 'female' 'female'\n 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'male' 'female' 'male' 'female' 'male' 'female' 'female' 'female'\n 'female' 'female' 'male' 'female' 'female' 'male'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 29\u001B[0m\n\u001B[0;32m     24\u001B[0m         merge_df[bool_feat] \u001B[38;5;241m=\u001B[39m merge_df[bool_feat]\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mint64)\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m merge_df\n\u001B[1;32m---> 29\u001B[0m \u001B[43mone_hot_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmerged_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msex\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# df = preprocess(merged_df)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[48], line 5\u001B[0m, in \u001B[0;36mone_hot_feature\u001B[1;34m(df, feat)\u001B[0m\n\u001B[0;32m      3\u001B[0m unique_values \u001B[38;5;241m=\u001B[39m df[feat]\u001B[38;5;241m.\u001B[39munique()\n\u001B[0;32m      4\u001B[0m enc \u001B[38;5;241m=\u001B[39m OneHotEncoder()\n\u001B[1;32m----> 5\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43menc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeat\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mc:\\programming\\python projects\\idpp2023\\idpp2023-kde-lab\\venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 140\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    143\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    144\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    145\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    146\u001B[0m         )\n",
      "File \u001B[1;32mc:\\programming\\python projects\\idpp2023\\idpp2023-kde-lab\\venv\\lib\\site-packages\\sklearn\\base.py:878\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    874\u001B[0m \u001B[38;5;66;03m# non-optimized default implementation; override when a better\u001B[39;00m\n\u001B[0;32m    875\u001B[0m \u001B[38;5;66;03m# method is possible for a given clustering algorithm\u001B[39;00m\n\u001B[0;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    877\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[1;32m--> 878\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[0;32m    879\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[0;32m    881\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[1;32mc:\\programming\\python projects\\idpp2023\\idpp2023-kde-lab\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:878\u001B[0m, in \u001B[0;36mOneHotEncoder.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    874\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparse_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparse\n\u001B[0;32m    876\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_infrequent_enabled()\n\u001B[1;32m--> 878\u001B[0m fit_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    880\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhandle_unknown\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_unknown\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_infrequent_enabled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    883\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    884\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_infrequent_enabled:\n\u001B[0;32m    885\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_infrequent_category_mapping(\n\u001B[0;32m    886\u001B[0m         fit_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_samples\u001B[39m\u001B[38;5;124m\"\u001B[39m], fit_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory_counts\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    887\u001B[0m     )\n",
      "File \u001B[1;32mc:\\programming\\python projects\\idpp2023\\idpp2023-kde-lab\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:74\u001B[0m, in \u001B[0;36m_BaseEncoder._fit\u001B[1;34m(self, X, handle_unknown, force_all_finite, return_counts)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_n_features(X, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_feature_names(X, reset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 74\u001B[0m X_list, n_samples, n_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_X\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_ \u001B[38;5;241m=\u001B[39m n_features\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcategories \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32mc:\\programming\\python projects\\idpp2023\\idpp2023-kde-lab\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:46\u001B[0m, in \u001B[0;36m_BaseEncoder._check_X\u001B[1;34m(self, X, force_all_finite)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;124;03mPerform custom check_array:\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;124;03m- convert list of strings to object dtype\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     42\u001B[0m \n\u001B[0;32m     43\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mhasattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miloc\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndim\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;66;03m# if not a dataframe, do normal check_array validation\u001B[39;00m\n\u001B[1;32m---> 46\u001B[0m     X_temp \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39missubdtype(X_temp\u001B[38;5;241m.\u001B[39mdtype, np\u001B[38;5;241m.\u001B[39mstr_):\n\u001B[0;32m     48\u001B[0m         X \u001B[38;5;241m=\u001B[39m check_array(X, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mobject\u001B[39m, force_all_finite\u001B[38;5;241m=\u001B[39mforce_all_finite)\n",
      "File \u001B[1;32mc:\\programming\\python projects\\idpp2023\\idpp2023-kde-lab\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:902\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    900\u001B[0m     \u001B[38;5;66;03m# If input is 1D raise error\u001B[39;00m\n\u001B[0;32m    901\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m array\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 902\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    903\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected 2D array, got 1D array instead:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124marray=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    904\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    905\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour data has a single feature or array.reshape(1, -1) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    906\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mif it contains a single sample.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(array)\n\u001B[0;32m    907\u001B[0m         )\n\u001B[0;32m    909\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype_numeric \u001B[38;5;129;01mand\u001B[39;00m array\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUSV\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    910\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    911\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumeric\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not compatible with arrays of bytes/strings.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    912\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConvert your data to numeric values explicitly instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    913\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Expected 2D array, got 1D array instead:\narray=['female' 'female' 'female' 'male' 'female' 'female' 'female' 'male'\n 'male' 'female' 'female' 'female' 'male' 'male' 'female' 'female' 'male'\n 'female' 'female' 'male' 'female' 'female' 'female' 'female' 'male'\n 'female' 'male' 'female' 'male' 'male' 'male' 'male' 'male' 'female'\n 'female' 'male' 'male' 'male' 'female' 'female' 'female' 'female'\n 'female' 'male' 'male' 'female' 'female' 'male' 'male' 'female' 'female'\n 'female' 'male' 'female' 'male' 'male' 'female' 'male' 'male' 'female'\n 'female' 'male' 'male' 'male' 'female' 'female' 'female' 'male' 'female'\n 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'male' 'female'\n 'male' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'male'\n 'female' 'female' 'female' 'female' 'female' 'male' 'female' 'female'\n 'male' 'female' 'female' 'female' 'female' 'female' 'female' 'male'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'male'\n 'male' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'male' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'female' 'female' 'female' 'female' 'male' 'female'\n 'male' 'female' 'male' 'male' 'female' 'female' 'male' 'female' 'female'\n 'female' 'male' 'female' 'female' 'male' 'female' 'male' 'female' 'male'\n 'male' 'female' 'female' 'female' 'female' 'female' 'female' 'male'\n 'male' 'male' 'male' 'male' 'female' 'female' 'male' 'female' 'male'\n 'female' 'male' 'female' 'female' 'male' 'female' 'female' 'male'\n 'female' 'male' 'female' 'male' 'male' 'male' 'female' 'female' 'female'\n 'female' 'female' 'female' 'male' 'female' 'male' 'female' 'female'\n 'female' 'male' 'female' 'female' 'male' 'male' 'female' 'female'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'female' 'female' 'female' 'female' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'male' 'female'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'male' 'female' 'male' 'female' 'female' 'male'\n 'female' 'female' 'female' 'male' 'male' 'female' 'male' 'male' 'female'\n 'female' 'male' 'female' 'female' 'female' 'female' 'male' 'female'\n 'female' 'male' 'female' 'female' 'female' 'male' 'female' 'male'\n 'female' 'male' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'female' 'male' 'female' 'female' 'female' 'male'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'male' 'female' 'female' 'female' 'female' 'male'\n 'female' 'male' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'male'\n 'female' 'male' 'male' 'female' 'female' 'female' 'female' 'male'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'female' 'female' 'female' 'male' 'male' 'female' 'female' 'male'\n 'female' 'male' 'male' 'male' 'male' 'female' 'female' 'female' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'female' 'male' 'female' 'female' 'male' 'female' 'male' 'female' 'male'\n 'female' 'female' 'female' 'male' 'female' 'male' 'female' 'male'\n 'female' 'female' 'female' 'male' 'female' 'female' 'female' 'male'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'male' 'female' 'female' 'female' 'female' 'female'\n 'male' 'female' 'male' 'female' 'female' 'male' 'female' 'female'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'female' 'male' 'female' 'female' 'female'\n 'male' 'female' 'male' 'female' 'male' 'female' 'male' 'male' 'female'\n 'female' 'male' 'female' 'female' 'female' 'female' 'female' 'female'\n 'female' 'female' 'male' 'male' 'female' 'female' 'male' 'female'\n 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n 'male' 'male' 'female' 'male' 'female' 'male' 'female' 'female' 'female'\n 'female' 'female' 'male' 'female' 'female' 'male'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def one_hot_feature(df, feat):\n",
    "    unique_values = df[feat].unique()\n",
    "    enc = OneHotEncoder()\n",
    "    out = enc.fit_transform(df[feat])\n",
    "    return out\n",
    "\n",
    "def preprocess(merge_df):\n",
    "    merge_df = merge_df.copy()\n",
    "    sex_map = {\"male\": 1, \"female\": 2}\n",
    "    merged_df[\"sex\"] = merged_df[\"sex\"].map(sex_map)\n",
    "\n",
    "    residence_map = {\"Towns\": 1, \"Rural Area\": 2, \"Cities\": 3}\n",
    "    merged_df[\"residence_classification\"] = merged_df[\"residence_classification\"].map(residence_map)\n",
    "\n",
    "    ethnicity_map = {\"Caucasian\": 1, np.nan: 0}\n",
    "    merged_df[\"ethnicity\"] = merged_df[\"ethnicity\"].map(ethnicity_map)\n",
    "\n",
    "    centre_map = {\"pavia\": 1, \"turin\": 2}\n",
    "    merged_df[\"ethnicity\"] = merged_df[\"ethnicity\"].map(centre_map)\n",
    "    bool_features = ['ms_in_pediatric_age', 'spinal_cord_symptom', 'brainstem_symptom',\n",
    "                     'eye_symptom', 'supratentorial_symptom']\n",
    "    for bool_feat in bool_features:\n",
    "        merge_df[bool_feat] = merge_df[bool_feat].astype(np.int64)\n",
    "\n",
    "\n",
    "    return merge_df\n",
    "\n",
    "one_hot_feature(merged_df, \"sex\")\n",
    "# df = preprocess(merged_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysing dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "missing_values = ((merged_df.isnull().sum() * 100 / len(merged_df)).sort_values(ascending=False))\n",
    "print(\"Missing value rate:\\n\", missing_values.to_string())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_values = ['sex', 'residence_classification', 'ethnicity',\n",
    "       'ms_in_pediatric_age', 'age_at_onset', 'diagnostic_delay',\n",
    "       'spinal_cord_symptom', 'brainstem_symptom', 'eye_symptom',\n",
    "       'supratentorial_symptom', 'other_symptoms', 'centre',\n",
    "       'time_since_onset', 'outcome_occurred', 'outcome_time']\n",
    "# sns.pairplot(merged_df[static_values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4)\n",
    "for ax, feat in zip(axes.flat, [\"outcome_time\",\"age_at_onset\", \"time_since_onset\",\"diagnostic_delay\"]):\n",
    "    sns.boxplot(merged_df, x=\"outcome_occurred\", y=feat, ax=ax)\n",
    "    ax.set(title=f\"{feat}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merged_df = merged_df.replace({np.nan: None})\n",
    "# def calc_len(x):\n",
    "#     if x is None:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return len(x)\n",
    "#\n",
    "# plot_df = pd.DataFrame(merged_df[[ID_FEAT, \"outcome_occurred\", \"delta_evoked_potential_time0\"]])\n",
    "# plot_df[\"delta_evoked_potential_time0_na\"] = plot_df[\"delta_evoked_potential_time0\"].isna()\n",
    "# plot_df[\"delta_evoked_potential_time0_len\"] = plot_df[\"delta_evoked_potential_time0\"].apply(calc_len)\n",
    "#\n",
    "# sns.countplot(x='delta_evoked_potential_time0_len', data=plot_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values = ['outcome_occurred', \"outcome_time\",'ms_in_pediatric_age', 'spinal_cord_symptom', 'brainstem_symptom', 'eye_symptom', 'supratentorial_symptom']\n",
    "corr_mat = merged_df[values].corr()\n",
    "f, ax = plt.subplots()\n",
    "sns.heatmap(corr_mat,  vmax=1, cmap=\"viridis\", square=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def seasonal_plot(X, y, period, freq, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n",
    "    ax = sns.lineplot(\n",
    "        x=freq,\n",
    "        y=y,\n",
    "        hue=period,\n",
    "        data=X,\n",
    "        ci=False,\n",
    "        ax=ax,\n",
    "        palette=palette,\n",
    "        legend=False,\n",
    "    )\n",
    "    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n",
    "    for line, name in zip(ax.lines, X[period].unique()):\n",
    "        y_ = line.get_ydata()[-1]\n",
    "        ax.annotate(\n",
    "            name,\n",
    "            xy=(1, y_),\n",
    "            xytext=(6, 0),\n",
    "            color=line.get_color(),\n",
    "            xycoords=ax.get_yaxis_transform(),\n",
    "            textcoords=\"offset points\",\n",
    "            size=14,\n",
    "            va=\"center\",\n",
    "        )\n",
    "    return ax\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}